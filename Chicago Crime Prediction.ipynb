{
    "nbformat_minor": 1, 
    "cells": [
        {
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20190306190101-0000\nKERNEL_ID = fba7ff55-2aba-4464-9b88-0dc934c0c187\nCollecting wget\n  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\nBuilding wheels for collected packages: wget\n  Running setup.py bdist_wheel for wget ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /home/spark/shared/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\nSuccessfully built wget\n\u001b[31mtensorflow 1.3.0 requires tensorflow-tensorboard<0.2.0,>=0.1.0, which is not installed.\u001b[0m\n\u001b[31mpyspark 2.3.0 requires py4j==0.10.6, which is not installed.\u001b[0m\nInstalling collected packages: wget\nSuccessfully installed wget-3.2\n"
                }
            ], 
            "source": "!pip install wget"
        }, 
        {
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Collecting tensorflow\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/f2/0931c194bb98398017d52c94ee30e5e1a4082ab6af76e204856ff1fdb33e/tensorflow-1.13.1-cp35-cp35m-manylinux1_x86_64.whl (92.5MB)\n\u001b[K    100% |################################| 92.5MB 238kB/s eta 0:00:01\n\u001b[?25hCollecting six>=1.10.0 (from tensorflow)\n  Downloading https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\nCollecting tensorflow-estimator<1.14.0rc0,>=1.13.0 (from tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n\u001b[K    100% |################################| 368kB 4.8MB/s eta 0:00:01\n\u001b[?25hCollecting absl-py>=0.1.6 (from tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/bc/ab68120d1d89ae23b694a55fe2aece2f91194313b71f9b05a80b32d3c24b/absl-py-0.7.0.tar.gz (96kB)\n\u001b[K    100% |################################| 102kB 4.2MB/s ta 0:00:01\n\u001b[?25hCollecting termcolor>=1.1.0 (from tensorflow)\n  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\nCollecting wheel>=0.26 (from tensorflow)\n  Downloading https://files.pythonhosted.org/packages/96/ba/a4702cbb6a3a485239fbe9525443446203f00771af9ac000fa3ef2788201/wheel-0.33.1-py2.py3-none-any.whl\nCollecting keras-preprocessing>=1.0.5 (from tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/bf/0315ef6a9fd3fc2346e85b0ff1f5f83ca17073f2c31ac719ab2e4da0d4a3/Keras_Preprocessing-1.0.9-py2.py3-none-any.whl (59kB)\n\u001b[K    100% |################################| 61kB 695kB/s eta 0:00:01\n\u001b[?25hCollecting gast>=0.2.0 (from tensorflow)\n  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\nCollecting numpy>=1.13.3 (from tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/18/4f013c3c3051f4e0ffbaa4bf247050d6d5e527fe9cb1907f5975b172f23f/numpy-1.16.2-cp35-cp35m-manylinux1_x86_64.whl (17.2MB)\n\u001b[K    100% |################################| 17.2MB 953kB/s eta 0:00:01\n\u001b[?25hCollecting tensorboard<1.14.0,>=1.13.0 (from tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n\u001b[K    100% |################################| 3.2MB 3.0MB/s eta 0:00:01\n\u001b[?25hCollecting grpcio>=1.8.6 (from tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/fd/e6696e5b115f328c382dd88414168e2b918cb7153b59dc9228d3c15e356c/grpcio-1.19.0-cp35-cp35m-manylinux1_x86_64.whl (10.8MB)\n\u001b[K    100% |################################| 10.8MB 1.6MB/s eta 0:00:01\n\u001b[?25hCollecting astor>=0.6.0 (from tensorflow)\n  Downloading https://files.pythonhosted.org/packages/35/6b/11530768cac581a12952a2aad00e1526b89d242d0b9f59534ef6e6a1752f/astor-0.7.1-py2.py3-none-any.whl\nCollecting protobuf>=3.6.1 (from tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/b4/0acb16276b92d0dabe3e97bf361b5ff9922d2071c497b92dde4741d4eeb4/protobuf-3.7.0-cp35-cp35m-manylinux1_x86_64.whl (1.2MB)\n\u001b[K    100% |################################| 1.2MB 3.8MB/s eta 0:00:01\n\u001b[?25hCollecting keras-applications>=1.0.6 (from tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/85/64c82949765cfb246bbdaf5aca2d55f400f792655927a017710a78445def/Keras_Applications-1.0.7-py2.py3-none-any.whl (51kB)\n\u001b[K    100% |################################| 61kB 2.6MB/s eta 0:00:01\n\u001b[?25hCollecting mock>=2.0.0 (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/35/f187bdf23be87092bd0f1200d43d23076cee4d0dec109f195173fd3ebc79/mock-2.0.0-py2.py3-none-any.whl (56kB)\n\u001b[K    100% |################################| 61kB 2.3MB/s eta 0:00:01\n\u001b[?25hCollecting markdown>=2.6.8 (from tensorboard<1.14.0,>=1.13.0->tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/6b/5600647404ba15545ec37d2f7f58844d690baf2f81f3a60b862e48f29287/Markdown-3.0.1-py2.py3-none-any.whl (89kB)\n\u001b[K    100% |################################| 92kB 3.7MB/s eta 0:00:01\n\u001b[?25hCollecting werkzeug>=0.11.15 (from tensorboard<1.14.0,>=1.13.0->tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/c4/12e3e56473e52375aa29c4764e70d1b8f3efa6682bef8d0aae04fe335243/Werkzeug-0.14.1-py2.py3-none-any.whl (322kB)\n\u001b[K    100% |################################| 327kB 4.1MB/s eta 0:00:01\n\u001b[?25hCollecting setuptools (from protobuf>=3.6.1->tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/6a/4b2fcefd2ea0868810e92d519dacac1ddc64a2e53ba9e3422c3b62b378a6/setuptools-40.8.0-py2.py3-none-any.whl (575kB)\n\u001b[K    100% |################################| 583kB 3.9MB/s eta 0:00:01\n\u001b[?25hCollecting h5py (from keras-applications>=1.0.6->tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/77/c4933e12dca0f61bcdafc207c7532e1250b8d12719459fd85132f3daa9fd/h5py-2.9.0-cp35-cp35m-manylinux1_x86_64.whl (2.8MB)\n\u001b[K    100% |################################| 2.8MB 3.0MB/s eta 0:00:01\n\u001b[?25hCollecting pbr>=0.11 (from mock>=2.0.0->tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/09/12fe9a14237a6b7e0ba3a8d6fcf254bf4b10ec56a0185f73d651145e9222/pbr-5.1.3-py2.py3-none-any.whl (107kB)\n\u001b[K    100% |################################| 112kB 4.1MB/s eta 0:00:01\n\u001b[?25hBuilding wheels for collected packages: absl-py, termcolor, gast\n  Running setup.py bdist_wheel for absl-py ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /home/spark/shared/.cache/pip/wheels/90/db/f8/2c3101f72ef1ad434e4662853174126ce30201a3e163dcbeca\n  Running setup.py bdist_wheel for termcolor ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /home/spark/shared/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n  Running setup.py bdist_wheel for gast ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /home/spark/shared/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\nSuccessfully built absl-py termcolor gast\n\u001b[31mpyspark 2.3.0 requires py4j==0.10.6, which is not installed.\u001b[0m\nInstalling collected packages: six, pbr, mock, numpy, absl-py, tensorflow-estimator, termcolor, wheel, keras-preprocessing, gast, markdown, setuptools, protobuf, grpcio, werkzeug, tensorboard, astor, h5py, keras-applications, tensorflow\nSuccessfully installed absl-py-0.7.0 astor-0.7.1 gast-0.2.2 grpcio-1.19.0 h5py-2.9.0 keras-applications-1.0.7 keras-preprocessing-1.0.9 markdown-3.0.1 mock-2.0.0 numpy-1.16.2 pbr-5.1.3 protobuf-3.7.0 setuptools-40.8.0 six-1.12.0 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0 termcolor-1.1.0 werkzeug-0.14.1 wheel-0.33.1\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3/__pycache__ already exists. Specify --upgrade to force replacement.\u001b[0m\n"
                }
            ], 
            "source": "!pip install tensorflow"
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Collecting py4j==0.10.6\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4a/08/162710786239aa72bd72bb46c64f2b02f54250412ba928cb373b30699139/py4j-0.10.6-py2.py3-none-any.whl (189kB)\n\u001b[K    100% |################################| 194kB 4.3MB/s eta 0:00:01\n\u001b[?25hInstalling collected packages: py4j\nSuccessfully installed py4j-0.10.6\n"
                }
            ], 
            "source": "!pip install py4j==0.10.6"
        }, 
        {
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Collecting tensorflow\n  Using cached https://files.pythonhosted.org/packages/ca/f2/0931c194bb98398017d52c94ee30e5e1a4082ab6af76e204856ff1fdb33e/tensorflow-1.13.1-cp35-cp35m-manylinux1_x86_64.whl\nCollecting tensorflow-estimator<1.14.0rc0,>=1.13.0 (from tensorflow)\n  Using cached https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl\nCollecting tensorboard<1.14.0,>=1.13.0 (from tensorflow)\n  Using cached https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl\nCollecting numpy>=1.13.3 (from tensorflow)\n  Using cached https://files.pythonhosted.org/packages/e3/18/4f013c3c3051f4e0ffbaa4bf247050d6d5e527fe9cb1907f5975b172f23f/numpy-1.16.2-cp35-cp35m-manylinux1_x86_64.whl\nCollecting keras-applications>=1.0.6 (from tensorflow)\n  Using cached https://files.pythonhosted.org/packages/90/85/64c82949765cfb246bbdaf5aca2d55f400f792655927a017710a78445def/Keras_Applications-1.0.7-py2.py3-none-any.whl\nCollecting grpcio>=1.8.6 (from tensorflow)\n  Using cached https://files.pythonhosted.org/packages/0e/fd/e6696e5b115f328c382dd88414168e2b918cb7153b59dc9228d3c15e356c/grpcio-1.19.0-cp35-cp35m-manylinux1_x86_64.whl\nCollecting wheel>=0.26 (from tensorflow)\n  Using cached https://files.pythonhosted.org/packages/96/ba/a4702cbb6a3a485239fbe9525443446203f00771af9ac000fa3ef2788201/wheel-0.33.1-py2.py3-none-any.whl\nCollecting six>=1.10.0 (from tensorflow)\n  Using cached https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\nCollecting termcolor>=1.1.0 (from tensorflow)\nCollecting astor>=0.6.0 (from tensorflow)\n  Using cached https://files.pythonhosted.org/packages/35/6b/11530768cac581a12952a2aad00e1526b89d242d0b9f59534ef6e6a1752f/astor-0.7.1-py2.py3-none-any.whl\nCollecting gast>=0.2.0 (from tensorflow)\nCollecting protobuf>=3.6.1 (from tensorflow)\n  Using cached https://files.pythonhosted.org/packages/f0/b4/0acb16276b92d0dabe3e97bf361b5ff9922d2071c497b92dde4741d4eeb4/protobuf-3.7.0-cp35-cp35m-manylinux1_x86_64.whl\nCollecting absl-py>=0.1.6 (from tensorflow)\nCollecting keras-preprocessing>=1.0.5 (from tensorflow)\n  Using cached https://files.pythonhosted.org/packages/c0/bf/0315ef6a9fd3fc2346e85b0ff1f5f83ca17073f2c31ac719ab2e4da0d4a3/Keras_Preprocessing-1.0.9-py2.py3-none-any.whl\nCollecting mock>=2.0.0 (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow)\n  Using cached https://files.pythonhosted.org/packages/e6/35/f187bdf23be87092bd0f1200d43d23076cee4d0dec109f195173fd3ebc79/mock-2.0.0-py2.py3-none-any.whl\nCollecting markdown>=2.6.8 (from tensorboard<1.14.0,>=1.13.0->tensorflow)\n  Using cached https://files.pythonhosted.org/packages/7a/6b/5600647404ba15545ec37d2f7f58844d690baf2f81f3a60b862e48f29287/Markdown-3.0.1-py2.py3-none-any.whl\nCollecting werkzeug>=0.11.15 (from tensorboard<1.14.0,>=1.13.0->tensorflow)\n  Using cached https://files.pythonhosted.org/packages/20/c4/12e3e56473e52375aa29c4764e70d1b8f3efa6682bef8d0aae04fe335243/Werkzeug-0.14.1-py2.py3-none-any.whl\nCollecting h5py (from keras-applications>=1.0.6->tensorflow)\n  Using cached https://files.pythonhosted.org/packages/4c/77/c4933e12dca0f61bcdafc207c7532e1250b8d12719459fd85132f3daa9fd/h5py-2.9.0-cp35-cp35m-manylinux1_x86_64.whl\nCollecting setuptools (from protobuf>=3.6.1->tensorflow)\n  Using cached https://files.pythonhosted.org/packages/d1/6a/4b2fcefd2ea0868810e92d519dacac1ddc64a2e53ba9e3422c3b62b378a6/setuptools-40.8.0-py2.py3-none-any.whl\nCollecting pbr>=0.11 (from mock>=2.0.0->tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow)\n  Using cached https://files.pythonhosted.org/packages/14/09/12fe9a14237a6b7e0ba3a8d6fcf254bf4b10ec56a0185f73d651145e9222/pbr-5.1.3-py2.py3-none-any.whl\nInstalling collected packages: six, pbr, mock, numpy, absl-py, tensorflow-estimator, grpcio, wheel, markdown, werkzeug, setuptools, protobuf, tensorboard, h5py, keras-applications, termcolor, astor, gast, keras-preprocessing, tensorflow\nSuccessfully installed absl-py-0.7.0 astor-0.7.1 gast-0.2.2 grpcio-1.19.0 h5py-2.9.0 keras-applications-1.0.7 keras-preprocessing-1.0.9 markdown-3.0.1 mock-2.0.0 numpy-1.16.2 pbr-5.1.3 protobuf-3.7.0 setuptools-40.8.0 six-1.12.0 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0 termcolor-1.1.0 werkzeug-0.14.1 wheel-0.33.1\n"
                }
            ], 
            "source": "!pip install --upgrade tensorflow"
        }, 
        {
            "execution_count": 5, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Collecting pyspark==2.3.0\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/58/49/45370cc153a6adcf2c304a3c06e801ed3c9650d0f852e7fde04bd8ffb534/pyspark-2.3.0.tar.gz (211.9MB)\n\u001b[K    100% |################################| 211.9MB 107kB/s eta 0:00:01\n\u001b[?25hCollecting py4j==0.10.6 (from pyspark==2.3.0)\n  Using cached https://files.pythonhosted.org/packages/4a/08/162710786239aa72bd72bb46c64f2b02f54250412ba928cb373b30699139/py4j-0.10.6-py2.py3-none-any.whl\nBuilding wheels for collected packages: pyspark\n  Running setup.py bdist_wheel for pyspark ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /home/spark/shared/.cache/pip/wheels/d9/db/ff/e6f3a8a564163ea64bc2072357e77b3404d10f91be48352796\nSuccessfully built pyspark\nInstalling collected packages: py4j, pyspark\nSuccessfully installed py4j-0.10.6 pyspark-2.3.0\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3/py4j already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3/py4j-0.10.6.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3/share already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3/bin already exists. Specify --upgrade to force replacement.\u001b[0m\n"
                }
            ], 
            "source": "!pip install pyspark==2.3.0"
        }, 
        {
            "execution_count": 6, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Collecting pyspark==2.3.0\nCollecting py4j==0.10.6 (from pyspark==2.3.0)\n  Using cached https://files.pythonhosted.org/packages/4a/08/162710786239aa72bd72bb46c64f2b02f54250412ba928cb373b30699139/py4j-0.10.6-py2.py3-none-any.whl\nInstalling collected packages: py4j, pyspark\nSuccessfully installed py4j-0.10.6 pyspark-2.3.0\n"
                }
            ], 
            "source": "!pip install --upgrade pyspark==2.3.0"
        }, 
        {
            "execution_count": 7, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 7, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[Row(ID=10642781, Case Number='HZ394208', Date='08/15/2016 11:15:00 AM', Block='061XX W GRAND AVE', IUCR='0460', Primary Type='BATTERY', Description='SIMPLE', Location Description='APARTMENT', Arrest=False, Domestic=False, Beat=2512, District=25, Ward=29, Community Area=19, FBI Code='08B', X Coordinate=1134907, Y Coordinate=1914405, Year=2016, Updated On='02/10/2018 03:50:01 PM', Latitude=41.921319589, Longitude=-87.779748093, Location='(41.921319589, -87.779748093)'),\n Row(ID=10642782, Case Number='HZ394162', Date='08/16/2016 10:17:00 AM', Block='009XX W 58TH ST', IUCR='0820', Primary Type='THEFT', Description='$500 AND UNDER', Location Description='SIDEWALK', Arrest=False, Domestic=True, Beat=712, District=7, Ward=16, Community Area=68, FBI Code='06', X Coordinate=1171050, Y Coordinate=1866373, Year=2016, Updated On='02/10/2018 03:50:01 PM', Latitude=41.788798848, Longitude=-87.648357712, Location='(41.788798848, -87.648357712)'),\n Row(ID=10642783, Case Number='HZ394180', Date='08/15/2016 10:00:00 PM', Block='070XX S ADA ST', IUCR='0890', Primary Type='THEFT', Description='FROM BUILDING', Location Description='RESIDENCE', Arrest=False, Domestic=False, Beat=734, District=7, Ward=17, Community Area=67, FBI Code='06', X Coordinate=1168542, Y Coordinate=1858171, Year=2016, Updated On='02/10/2018 03:50:01 PM', Latitude=41.766346009, Longitude=-87.657789909, Location='(41.766346009, -87.657789909)'),\n Row(ID=10642787, Case Number='HZ394193', Date='08/11/2016 09:00:00 AM', Block='023XX W MOFFAT ST', IUCR='0917', Primary Type='MOTOR VEHICLE THEFT', Description='CYCLE, SCOOTER, BIKE W-VIN', Location Description='STREET', Arrest=False, Domestic=False, Beat=1434, District=14, Ward=1, Community Area=22, FBI Code='07', X Coordinate=1160677, Y Coordinate=1912313, Year=2016, Updated On='02/10/2018 03:50:01 PM', Latitude=41.915083455, Longitude=-87.685120419, Location='(41.915083455, -87.685120419)'),\n Row(ID=10642788, Case Number='HZ394160', Date='08/16/2016 09:20:00 AM', Block='013XX W LAKE ST', IUCR='0460', Primary Type='BATTERY', Description='SIMPLE', Location Description='COMMERCIAL / BUSINESS OFFICE', Arrest=False, Domestic=False, Beat=1224, District=12, Ward=27, Community Area=28, FBI Code='08B', X Coordinate=1167667, Y Coordinate=1901571, Year=2016, Updated On='02/10/2018 03:50:01 PM', Latitude=41.885458855, Longitude=-87.659749904, Location='(41.885458855, -87.659749904)')]"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "# The code was removed by Watson Studio for sharing."
        }, 
        {
            "execution_count": 8, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "root\n |-- ID: integer (nullable = true)\n |-- Case Number: string (nullable = true)\n |-- Date: string (nullable = true)\n |-- Block: string (nullable = true)\n |-- IUCR: string (nullable = true)\n |-- Primary Type: string (nullable = true)\n |-- Description: string (nullable = true)\n |-- Location Description: string (nullable = true)\n |-- Arrest: boolean (nullable = true)\n |-- Domestic: boolean (nullable = true)\n |-- Beat: integer (nullable = true)\n |-- District: integer (nullable = true)\n |-- Ward: integer (nullable = true)\n |-- Community Area: integer (nullable = true)\n |-- FBI Code: string (nullable = true)\n |-- X Coordinate: integer (nullable = true)\n |-- Y Coordinate: integer (nullable = true)\n |-- Year: integer (nullable = true)\n |-- Updated On: string (nullable = true)\n |-- Latitude: double (nullable = true)\n |-- Longitude: double (nullable = true)\n |-- Location: string (nullable = true)\n\n"
                }
            ], 
            "source": "df_data_1.printSchema()"
        }, 
        {
            "execution_count": 9, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 9, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "6140717"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "df_data_1.dropna(how=\"any\").count()"
        }, 
        {
            "execution_count": 10, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 10, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "35"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "df_data_1.select('Primary Type').distinct().count()"
        }, 
        {
            "execution_count": 11, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 11, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "381"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "df_data_1.select('Description').distinct().count()"
        }, 
        {
            "execution_count": 12, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 12, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "180"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "df_data_1.select('Location Description').distinct().count()"
        }, 
        {
            "execution_count": 13, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 13, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "19"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "df_data_1.select('Year').distinct().count()"
        }, 
        {
            "execution_count": 14, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 14, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "403"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "df_data_1.select('IUCR').distinct().count()"
        }, 
        {
            "execution_count": 15, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 15, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "304"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "df_data_1.select('Beat').distinct().count()"
        }, 
        {
            "execution_count": 16, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 16, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "25"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "df_data_1.select('District').distinct().count()"
        }, 
        {
            "execution_count": 17, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 17, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "51"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "df_data_1.select('Ward').distinct().count()"
        }, 
        {
            "execution_count": 18, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 18, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "79"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "df_data_1.select('Community Area').distinct().count()"
        }, 
        {
            "execution_count": 19, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 19, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "26"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "df_data_1.select('FBI Code').distinct().count()"
        }, 
        {
            "execution_count": 20, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Number of training records: 4911711\nNumber of testing records : 1106642\nNumber of prediction records : 122364\n"
                }
            ], 
            "source": "splitted_data = df_data_1.randomSplit([0.8, 0.18, 0.02], 24)\ntrain_data = splitted_data[0]\ntest_data = splitted_data[1]\npredict_data = splitted_data[2]\n\nprint(\"Number of training records: \" + str(train_data.dropna(how=\"any\").count()))\nprint(\"Number of testing records : \" + str(test_data.dropna(how=\"any\").count()))\nprint(\"Number of prediction records : \" + str(predict_data.dropna(how=\"any\").count()))"
        }, 
        {
            "execution_count": 21, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml.feature import OneHotEncoder, StringIndexer, IndexToString, VectorAssembler\nfrom pyspark.ml.classification import RandomForestClassifier, DecisionTreeClassifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml import Pipeline, Model\nfrom pyspark.ml.tuning import ParamGridBuilder;"
        }, 
        {
            "execution_count": 29, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "stringIndexer_label = StringIndexer(inputCol=\"Primary Type\", outputCol=\"label\", handleInvalid=\"keep\").fit(df_data_1.dropna(how=\"any\"))\nstringIndexer_fbi = StringIndexer(inputCol=\"FBI Code\", outputCol=\"FBI_Code_IX\", handleInvalid=\"keep\")"
        }, 
        {
            "execution_count": 30, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "vectorAssembler_features = VectorAssembler(inputCols=[\"FBI_Code_IX\", \"District\", \"Ward\", \"Community Area\", \"Year\"], outputCol=\"features\")"
        }, 
        {
            "execution_count": 31, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")"
        }, 
        {
            "execution_count": 32, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=stringIndexer_label.labels)"
        }, 
        {
            "execution_count": 33, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "pipeline_rf = Pipeline(stages=[stringIndexer_label, stringIndexer_fbi, vectorAssembler_features, rf, labelConverter])"
        }, 
        {
            "execution_count": 34, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "root\n |-- ID: integer (nullable = true)\n |-- Case Number: string (nullable = true)\n |-- Date: string (nullable = true)\n |-- Block: string (nullable = true)\n |-- IUCR: string (nullable = true)\n |-- Primary Type: string (nullable = true)\n |-- Description: string (nullable = true)\n |-- Location Description: string (nullable = true)\n |-- Arrest: boolean (nullable = true)\n |-- Domestic: boolean (nullable = true)\n |-- Beat: integer (nullable = true)\n |-- District: integer (nullable = true)\n |-- Ward: integer (nullable = true)\n |-- Community Area: integer (nullable = true)\n |-- FBI Code: string (nullable = true)\n |-- X Coordinate: integer (nullable = true)\n |-- Y Coordinate: integer (nullable = true)\n |-- Year: integer (nullable = true)\n |-- Updated On: string (nullable = true)\n |-- Latitude: double (nullable = true)\n |-- Longitude: double (nullable = true)\n |-- Location: string (nullable = true)\n\n"
                }
            ], 
            "source": "train_data.printSchema()"
        }, 
        {
            "execution_count": 36, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "model_rf = pipeline_rf.fit(train_data.dropna(how=\"any\"))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "predictions = model_rf.transform(test_data.dropna(how=\"any\"))\nevaluatorRF = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluatorRF.evaluate(predictions)\n\nprint(\"Accuracy = %g\" % accuracy)\nprint(\"Test Error = %g\" % (1.0 - accuracy))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from repository.mlrepositoryclient import MLRepositoryClient\nfrom repository.mlrepositoryartifact import MLRepositoryArtifact"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "ml_repository_client = MLRepositoryClient(service_path)\nml_repository_client.authorize(username, password)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "model_artifact = MLRepositoryArtifact(model_rf, training_data = train_data, name=\"Crime Line prediction\")"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "saved_model = ml_repository_client.models.save(model_artifact)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "saved_model.meta.available_props()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print (\"modelType: \" + saved_model.meta.prop(\"modelType\"))\nprint (\"trainingDataSchema: \" + str(saved_model.meta.prop(\"trainingDataSchema\")))\nprint (\"creationTime: \" + str(saved_model.meta.prop(\"creationTime\")))\nprint (\"modelVersionHref: \" + saved_model.meta.prop(\"modelVersionHref\"))\nprint (\"label: \" + saved_model.meta.prop(\"label\"))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "loadedModelArtifact = ml_repository_client.models.get(saved_model.uid)"
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark", 
            "name": "python3", 
            "language": "python3"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}